{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/analyticsindiamagazine/8a9015b7f8d12d5b414e89007d9b069a/starter_notebook_for_participants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEFmR5J_COHU"
   },
   "source": [
    "## Import Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEZXHo2YVq2B"
   },
   "outputs": [],
   "source": [
    "#imports needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "# device = torch.device('cuda:0') # Use the MLT GPU when you can!\n",
    "device = torch.device('cpu')\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YosGM4uOCOHZ"
   },
   "outputs": [],
   "source": [
    "## change it to the unzip path of the downloaded dataset..\n",
    "data_folder = r'/home/gusmavko@GU.GU.SE/MovieScriptsParticipantsData/Scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MGrGh94COHh",
    "outputId": "6f3c2d5e-d458-4909-afcd-1f66475bab18"
   },
   "outputs": [],
   "source": [
    "all_files = os.listdir(data_folder)\n",
    "print('Total Number of Files :', len(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3a0Y3Q0Vq2L"
   },
   "source": [
    "# Read Train Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nps340zYVq2M",
    "outputId": "2a0d42cd-f6b2-4884-b2b7-3547c4d18a1a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIUam5eVVq2Q",
    "outputId": "ddd148da-7dfc-4e25-bca7-e39bf1c506b6"
   },
   "outputs": [],
   "source": [
    "train_df.Labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYZ9t9U8Vq2W",
    "outputId": "92198597-c824-4597-9a5b-c74df7552130"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/home/gusmavko@GU.GU.SE/MovieScriptsParticipantsData/Test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3x3bITvVq2Y",
    "outputId": "c24ee160-1caa-4a07-db6c-6c0b3c3d9fe3"
   },
   "outputs": [],
   "source": [
    "train_df.Labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DqMPYhrVq2d"
   },
   "outputs": [],
   "source": [
    "## let's read the text scripts in the train and test dataframes..\n",
    "import string\n",
    "\n",
    "train_df['text'] = [open(data_folder + os.sep + file, \"r\").read() for file in train_df['File_Name']]\n",
    "test_df['text'] = [open(data_folder + os.sep + file, \"r\").read() for file in test_df['File_Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "def clean_text(text):\n",
    "    tokenized_text = []\n",
    "    sw = stopwords.words(('english'))\n",
    "    # remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    # remove whitespaces and convert to lowercase \n",
    "    text = ' '.join(text.split()).lower() \n",
    "    \n",
    "    no_stopword_text = [word for word in text.split() if not word in sw]\n",
    "     \n",
    "    \n",
    "#    for tok in text:\n",
    "#        tokens = word_tokenize(tok)\n",
    "#        for each_token in tokens:\n",
    "#            if each_token in sw:\n",
    "#                tokens.remove(each_token)\n",
    "#            else:\n",
    "#                continue\n",
    "#        tokenized_text.append(tokens)\n",
    "    \n",
    "       \n",
    "    return ' '.join(no_stopword_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.head(7)\n",
    "#train_df['clean_text'] = train_df['text'].apply(lambda x: clean_text_text(x))\n",
    "#test_df['text']= test_df.text.apply(clean_text)\n",
    "train_df['clean_text'] = train_df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['clean_text'] = test_df['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(r'/home/gusmavko@GU.GU.SE/MovieScriptsParticipantsData/data/proc_training.csv', index = False, header=True,sep='\\t')\n",
    "test_df.to_csv(r'/home/gusmavko@GU.GU.SE/MovieScriptsParticipantsData/data/proc_testing.csv', index = False, header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "import spacy\n",
    "\n",
    "def dataloader(path):\n",
    "\n",
    "    ddir = path # '/home/gusmavko@GU.GU.SE/MovieScriptsParticipantsData/data/'\n",
    "    \n",
    "    FILE = Field(batch_first = True,\n",
    "                 lower = True)\n",
    "    \n",
    "    \n",
    "    LABEL = Field(batch_first = True,\n",
    "                  sequential = False,\n",
    "                   use_vocab = False)\n",
    "    \n",
    "    TEXT = Field(tokenize = 'spacy',\n",
    "                 tokenizer_language= 'en',\n",
    "                 batch_first = True,\n",
    "                 lower = True,\n",
    "                 use_vocab = True)    \n",
    "    \n",
    "    \n",
    "\n",
    "    fields = [('file',FILE),('genre',LABEL),('script', TEXT)]\n",
    "\n",
    "    train, test = TabularDataset.splits(path=path,\n",
    "                                        train = 'proc_training.csv',\n",
    "                                        test = 'proc_testing.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True,\n",
    "                                        csv_reader_params = {'delimiter': '\\t'})\n",
    "                                                             \n",
    "    \n",
    "    FILE.build_vocab(train, test)\n",
    "    \n",
    "    LABEL.build_vocab(train, test)\n",
    "    TEXT.build_vocab(train, test)\n",
    "    \n",
    "    \n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                   batch_size = 10,\n",
    "                                                   sort_within_batch = True,\n",
    "                                                   sort_key = lambda x: len(x.script),\n",
    "                                                   shuffle = True,\n",
    "                                                   device = device)\n",
    "    \n",
    "    return train_iter, test_iter, FILE, LABEL, TEXT\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter, FILE, LABEL, TEXT = dataloader('/home/gusmavko@GU.GU.SE/MovieScriptsParticipantsData/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_file = 'len of file is =', len(FILE.vocab)\n",
    "num_label = len(LABEL.vocab)\n",
    "num_script = len(TEXT.vocab)\n",
    "num_file\n",
    "num_script\n",
    "\n",
    "num_file,num_label, num_script\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "LABEL.vocab.stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 535M\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE 131M Oct 19 09:38 HATE.ipynb\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE 247K Mar 30  2020 Movie_Scripts_Sample_Submission.xlsx\r\n",
      "drwxr-xr-x. 2 gusmavko@GU.GU.SE domain users@GU.GU.SE 216K Mar 30  2020 Scripts\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE  16K Oct 19 09:53 StarterNotebookV2.ipynb\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE  13K Mar 30  2020 Test.csv\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE  31K Mar 30  2020 Train.csv\r\n",
      "drwxr-xr-x. 2 gusmavko@GU.GU.SE domain users@GU.GU.SE 4.0K Sep 23 10:07 data\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE 236M Sep 23 08:42 proc_testing.csv\r\n",
      "-rw-r--r--. 1 gusmavko@GU.GU.SE domain users@GU.GU.SE 166M Sep 23 08:59 proc_training.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZzXe4SqVq2x"
   },
   "source": [
    "# Import the Modeling Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow the sample submission format for your submission file\n",
    "\n",
    "#### Please verify the following before submitting your solution to avoid an invalid sibmission.\n",
    "\n",
    "1. The format of the file is excel(.xlsx)\n",
    "\n",
    "2. The file doesn’t contain additional styling elements such as bold headings or table borders\n",
    "\n",
    "3. The length of the submission exactly matches with that of the sample submission and test set\n",
    "\n",
    "4. The file name doesnot have any spaces or special characters\n",
    "\n",
    "5. All the columns are present"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Starter_Notebook_For_Participants.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
